{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28cd1fe2",
   "metadata": {},
   "source": [
    "# Fraud detection using Machine Learning\n",
    "\n",
    "# This project builds several models to classify credit card transactions as a fraud or non-fraud.\n",
    "# This is the first version of the code and the main version for the project.\n",
    "\n",
    "# Models Used:\n",
    "# - Logistic Regression\n",
    "# - Random Forest\n",
    "# - XGBoost\n",
    "# - Decision Tree\n",
    "\n",
    "# Evaluation Metrics:\n",
    "# - ROC AUC Score (primary metric)\n",
    "# - Precision, Recall, F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9958f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data\n",
    "import pandas as pd\n",
    "data = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5a1bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc67a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#information about the data\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472dea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check missing values\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b93f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for balance and visualise it\n",
    "print(data[\"Class\"].value_counts())\n",
    "\n",
    "import seaborn as sns\n",
    "sns.countplot(x = \"Class\", data = data)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title(\"Fraud transactions vs non-fraud transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad02e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the data\n",
    "#cant see the significance of time, so im dropping the time column\n",
    "data = data.drop([\"Time\"],axis = 1)\n",
    "\n",
    "#scale amount column using standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scale = StandardScaler()\n",
    "data[\"Amount\"] = scale.fit_transform(data[[\"Amount\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03805358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the data\n",
    "x = data.drop([\"Class\"], axis = 1)\n",
    "y = data[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c559b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y, random_state= 42, stratify= y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae9b7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#way 1: \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "models = {\"Logistic regression\":LogisticRegression(max_iter= 1000, class_weight= \"balanced\"),\n",
    "          \"Random forest\":RandomForestClassifier(),\n",
    "          \"XGBoost\":XGBClassifier(),\n",
    "          \"Decision tree\":DecisionTreeClassifier()}\n",
    "for modelname,model in models.items():\n",
    "    print(f\"{modelname.upper()} RESULTS:\")\n",
    "    model.fit(x_train,y_train)\n",
    "    y_predict = model.predict(x_test)\n",
    "    y_proba = model.predict_proba(x_test)[:,1] \n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    accuracy_roc = roc_auc_score(y_test, y_proba) #used roc cause accuracy is not the best for unbalanced data\n",
    "    cm = confusion_matrix(y_test, y_predict)\n",
    "    print(f\"Accuracy = {accuracy}\\nArea under curve score = {accuracy_roc}\\nConfusion matrix = {cm}\")\n",
    "    \n",
    "    #heat map to visualize the confusion matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f\"{modelname} Confusion matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "    #showing the other evaluation metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(f\"{modelname} Classification report:\")\n",
    "    print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3264b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drawing graphs for roc\n",
    "from sklearn.metrics import roc_curve\n",
    "for modelname,model in models.items():\n",
    "    x_axis_fp, y_axis_tp, _ = roc_curve(y_test, model.predict_proba(x_test)[:,1])\n",
    "    plt.plot( x_axis_fp, y_axis_tp, label = modelname)\n",
    "plt.xlabel(\"False positive\")\n",
    "plt.ylabel(\"True positive\")\n",
    "plt.title(\"ROC Curve for all models\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04fe25e",
   "metadata": {},
   "source": [
    "# conclusion\n",
    "# In this project, I built a machine learning fraud detection system using credit card fraud dataset from kaggle. \n",
    "# The dataset presented a problem due to its extreme class imbalance, with fraud transactions being less than 0.2% of the entire data.\n",
    "# I trained four models: Logistic regression, Decision tree, Random forest and XGBoost.\n",
    "# To address the extreme imbalance in the dataset, I used stratified sampling when train-test spliting the data to make sure that both training and testing sets had the same proportion of fraud and non-fraud cases as the original dataset.\n",
    "# To handle the imbalance further, I applied class weighting on Logistic regression model.\n",
    "# The models were primarily assessed using the ROC-AUC score, precision, recall and F1-score rather than accuracy alone because accuracy alone would be misleading since the dataset is highly skewed(imbalanced).\n",
    "# Amongst the models, XGBoost and Random Forest showed a strong performance by achieving high ROC-AUC scores and also balanced precision and recall values. Logistic regression with balanced class weights also performed competitively which can be added on to the benefit of interpretability.\n",
    "# Despite strong performances, no model had a perfect fraud detection. For instance, the best models correctly identified approximately 74% of fraudulent transactions, which is good but it still leaves room for undetected fraud. This highlights the trade-off between catching fraud and avoiding false alarms in real-world applications.\n",
    "# To further improve performance, future work will consider using other techniques.\n",
    "# Overall, this small project shows that machine learning provides a valuable foundation for detecting fraud in real-time fraud prevention systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
